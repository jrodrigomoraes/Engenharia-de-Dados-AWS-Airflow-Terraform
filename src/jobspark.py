# -*- coding: utf-8 -*-
"""JobSpark.ipynb

Automatically generated by Colab.
"""

!pip install boto3

#Importar as bibliotecas necessárias
from pyspark.sql import SparkSession
import getpass
from pyspark.sql.functions import col, expr, sequence, to_date, date_format, year, month, dayofmonth, dayofweek, quarter, lit, explode
from pyspark.sql.types import DateType
from pyspark.sql.functions import datediff
from dotenv import load_dotenv
import boto3
import os

load_dotenv()
aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')
aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')

#Criar a SparkSession (núcleo da aplicação PySpark)
spark = SparkSession.builder \
    .appName("Leitura PostgreSQL - Tabela Locacoes") \
    .config("spark.jars.packages", "org.postgresql:postgresql:42.6.0") \
    .getOrCreate()

#Definir as credenciais e parâmetros da conexão JDBC
host = "locadora-db.c2t602m2ee37.us-east-1.rds.amazonaws.com"
porta = "5432"
banco = "locadora"
usuario = "postgres"
senha = getpass.getpass("Digite a senha do PostgreSQL:")

#Função para ler as tabelas do PostgreSQL
def read_table(table_name):
    return spark.read.format("jdbc") \
        .option("url", f"jdbc:postgresql://{host}:{porta}/{banco}") \
        .option("dbtable", f"public.{table_name}") \
        .option("user", usuario) \
        .option("password", senha) \
        .option("driver", "org.postgresql.Driver") \
        .load()

#Carregar todas as tabelas com a função
df_locacoes = read_table("locacoes")
df_veiculos = read_table("veiculos")
df_despachantes = read_table("despachantes")
df_clientes = read_table("clientes")

#Mostrar os dados das tabelas
df_locacoes.show(10)
df_veiculos.show(10)
df_despachantes.show(10)
df_clientes.show(10)

#Criando as dimensões
dim_clientes = df_clientes.select(
    df_clientes["id"].alias("id_cliente"),
    "cpf", "cnh", "validade_cnh", "nome", "data_cadastro", "data_nascimento", "telefone", "status"
)

dim_veiculos = df_veiculos.select(
    df_veiculos["id"].alias("id_veiculo"),
    "data_aquisicao", "ano", "modelo", "placa", "status", "preco_diaria"
)

dim_despachantes = df_despachantes.select(
    df_despachantes["id"].alias("id_despachante"),
    "nome", "status", "filial"
)

#Criar a dimensão de tempo
dim_data = spark.range(1) \
    .withColumn("data", sequence(to_date(lit("2019-01-01")), to_date(lit("2025-12-31")))) \
    .select(explode(col("data")).alias("data"))

dim_data = dim_data \
    .withColumn("ano", year(col("data"))) \
    .withColumn("mes", month(col("data"))) \
    .withColumn("dia", dayofmonth(col("data"))) \
    .withColumn("nome_mes", date_format(col("data"), "MMMM")) \
    .withColumn("nome_dia_semana", date_format(col("data"), "EEEE")) \
    .withColumn("trimestre", quarter(col("data"))) \
    .withColumn("fim_de_semana", dayofweek(col("data")).isin([1, 7]).cast("boolean"))

#Criar o fato locacoes
fato_locacoes = df_locacoes.select(
    df_locacoes["id"].alias("id_locacao"),
    df_locacoes["cliente_id"].alias("id_cliente"),
    df_locacoes["veiculo_id"].alias("id_veiculo"),
    df_locacoes["despachante_id"].alias("id_despachante"),
    df_locacoes["data_locacao"],
    df_locacoes["data_entrega"],
    df_locacoes["valor_total"],
    datediff("data_entrega", "data_locacao").alias("dias_locados")
)

#Inicializa cliente S3
s3 = boto3.client(
    's3',
    aws_access_key_id=aws_access_key_id,
    aws_secret_access_key=aws_secret_access_key
)

#Função para salvar e fazer upload para o S3
def save_and_upload_to_s3(df, local_path, s3_path):
    df.write.mode("overwrite").parquet(local_path)
    s3.upload_file(local_path, "locadora-analitico-jrm", s3_path)
    print(f"Upload realizado com sucesso: {s3_path}")

#Salvar e fazer upload das dimensões
save_and_upload_to_s3(dim_clientes, "/content/dim_clientes", "dimensoes/dim_clientes")
save_and_upload_to_s3(dim_veiculos, "/content/dim_veiculos", "dimensoes/dim_veiculos")
save_and_upload_to_s3(dim_despachantes, "/content/dim_despachantes", "dimensoes/dim_despachantes")
save_and_upload_to_s3(dim_data, "/content/dim_tempo", "dimensoes/dim_tempo")

#Salvar e fazer upload do fato locações
fato_locacoes_particionado = fato_locacoes \
    .withColumn("ano", year("data_locacao")) \
    .withColumn("mes", month("data_locacao"))

save_and_upload_to_s3(fato_locacoes_particionado, "/content/fato_locacoes", "fato/fato_locacoes")

# üöó Projeto de Engenharia de Dados: Locadora de Ve√≠culos

Este projeto simula a cria√ß√£o de um pipeline de dados completo para uma empresa fict√≠cia de loca√ß√£o de ve√≠culos, utilizando tecnologias modernas de engenharia de dados.

## Objetivos

- Simular um ambiente anal√≠tico de uma locadora
- Criar uma base relacional com dados normalizados
- Realizar transforma√ß√µes e disponibilizar dados prontos para BI
- Automatizar o pipeline com Airflow e organizar a infraestrutura com Terraform

## Dados Utilizados

- **Fato:** Loca√ß√µes realizadas
- **Dimens√µes:** Clientes, ve√≠culos, despachantes, tempo
- Dados exportados no formato Parquet e armazenados no Amazon S3
- **Observa√ß√£o:** Os dados foram gerados artificialmente com a biblioteca Faker do Python para simular um cen√°rio realista.

## Arquitetura

Foi utilizado o PostgreSQL tanto de forma local (offline) quanto em uma inst√¢ncia no Amazon RDS. Ap√≥s a coleta e armazenamento dos dados, foi realizada uma transforma√ß√£o utilizando Apache Spark, com processamento adicional via Amazon Athena para responder perguntas de neg√≥cio. Os resultados das transforma√ß√µes foram armazenados no Amazon S3, tanto em seu formato bruto quanto em formatos otimizados para an√°lise com ferramentas de BI.

## Tecnologias

- **PySpark**: Transforma√ß√£o e limpeza dos dados, escrita em formato Parquet.
- **Airflow**: Orquestra√ß√£o dos pipelines de dados.
- **AWS S3**: Armazenamento em camadas (raw, staging e output).
- **Athena**: Consulta de dados diretamente do S3 e exporta√ß√£o de resultados.
- **Terraform**: Para provisionamento da infraestrutura em nuvem.
- **MongoDB**: Para identificar contratos de risco, que estavam no formato json. (√â uma etapa opcional)
- **QuickSight**: Ferramenta para visualiza√ß√£o dos resultados

## Perguntas que foram respondidas

Ap√≥s tratar os dados com PySpark, utilizei Athena para responder quest√µes sobre o neg√≥cio.

- Quais ve√≠culos foram locados em determinados per√≠odos?
- Quais despachantes locaram quais ve√≠culos?
- Qual faturamento por per√≠odo?
- Quais clientes locaram quais ve√≠culos?
- Tempo m√©dio de loca√ß√£o por modelo
- Cliente mais fiel (que mais alugou)

## Pipeline de Dados

O pipeline de dados seguiu o seguinte fluxo:
- **Ingest√£o**: Dados brutos extra√≠dos de um banco PostgreSQL local e do Amazon RDS.
- **Transforma√ß√£o**: Uso de PySpark para limpar, normalizar e criar tabelas fato e dimens√µes.
-  **Armazenamento**:
   - Camada `raw`: Dados brutos
   - Camada `staging`: Dados organizados por dimens√£o
   - Camada `output`: Resultados de queries salvos via Athena para BI
- **Orquestra√ß√£o**: Apache Airflow executando DAGs para automa√ß√£o e movimenta√ß√£o dos dados.
- **Identifica√ß√£o de Contratos de Risco**: Uso de MongoDB para visualiza√ß√£o de dados em arquivos semi-estruturados
- **Visualiza√ß√£o de Dados**: Utilizado ferramentas de BI(aqui pode ser a da sua prefer√™ncia) para visualizar as bases criadas com Athena.

## Failures

Detalhamento de um erro encontrado durante o projeto. Erros que podem acontecer em um pipeline real.

## Scripts

Os scripts est√£o organizados nas pastas `src`, `scripts` e `sql`. Seus objetivos incluem:
- Cria√ß√£o do Banco de Dados e Tabelas
- Cria√ß√£o de um `job Spark` para transforma√ß√£o dos dados
- Gera√ß√£o de dados com `Faker` e inser√ß√£o no banco
- Respostas a perguntas de neg√≥cio com queries SQL
- `DAGs do Airflow` para automatiza√ß√£o da movimenta√ß√£o entre camadas
- C√≥digo para leitura de contratos em JSON via `MongoDB` e identifica√ß√£o de contratos de risco

## Outputs

Dados prontos para ser utilizados em ferramentas de BI, optei por `Amazon QuickSight` por conta da integra√ß√£o com o ambiente AWS.

## Terraform

A infraestrutura b√°sica foi provisionada com Terraform, facilitando a automa√ß√£o e escalabilidade do ambiente anal√≠tico com boas pr√°ticas de Infrastructure as Code (IaC). Este projeto tamb√©m est√° sendo utilizado como parte do meu aprendizado de Terraform.

## Visualiza√ß√£o

Resultado das perguntas de neg√≥cio em dashboards. Apenas as imagens.
